{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning 教程\n",
    "\n",
    "主要参考：\n",
    "\n",
    "- [Training a Simple Neural Network, with PyTorch Data Loading](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html)\n",
    "- [From PyTorch to JAX: towards neural net frameworks that purify stateful code](https://sjmielke.com/jax-purify.htm)\n",
    "- [JAX vs Tensorflow vs Pytorch: Building a Variational Autoencoder (VAE)](https://theaisummer.com/jax-tensorflow-pytorch/)\n",
    "\n",
    "首先训练一个简单的神经网络。\n",
    "\n",
    "## Training a Simple Neural Network\n",
    "\n",
    "首先使用JAX，在MNIST上指定并训练一个简单的MLP。将使用PyTorch的数据加载API加载图像和标签（因为它非常棒，我们不需要另一个数据加载库）。\n",
    "\n",
    "当然，可以将JAX和与NumPy兼容的任何API一起使用，以使模型更加即插即用。本小节，仅出于说明目的，将不使用任何神经网络库或特殊的API来构建模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，定义超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "layer_sizes = [784, 512, 512, 10]\n",
    "param_scale = 0.1\n",
    "step_size = 0.01\n",
    "num_epochs = 8\n",
    "batch_size = 128\n",
    "n_targets = 10\n",
    "params = init_network_params(layer_sizes, random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后定义预测函数。请注意，是为单个图像示例定义的。将使用JAX的vmap功能自动处理mini-batchs，而不会影响性能。\n",
    "\n",
    "关于其中用到的logesumexp，可以参考[这里](https://blog.feedly.com/tricks-of-the-trade-logsumexp/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def predict(params, image):\n",
    "    # per-example predictions\n",
    "    activations = image\n",
    "    # 最后一层前的几层\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = jnp.dot(w, activations) + b\n",
    "        activations = relu(outputs)\n",
    "    # 最后一层\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = jnp.dot(final_w, activations) + final_b\n",
    "    return logits - logsumexp(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们检查一下适用于单个图像的预测功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# This works on single examples\n",
    "random_flattened_image = random.normal(random.PRNGKey(1), (28 * 28,))\n",
    "preds = predict(params, random_flattened_image)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid shapes!\n"
     ]
    }
   ],
   "source": [
    "# Doesn't work with a batch\n",
    "random_flattened_images = random.normal(random.PRNGKey(1), (10, 28 * 28))\n",
    "try:\n",
    "    preds = predict(params, random_flattened_images)\n",
    "except TypeError:\n",
    "    print('Invalid shapes!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "# Let's upgrade it to handle batches using `vmap`\n",
    "\n",
    "# Make a batched version of the `predict` function\n",
    "batched_predict = vmap(predict, in_axes=(None, 0))\n",
    "\n",
    "# `batched_predict` has the same call signature as `predict`\n",
    "batched_preds = batched_predict(params, random_flattened_images)\n",
    "print(batched_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们拥有定义神经网络并对其进行训练所需的所有要素。建立了auto-batched 版本的predict，可以在损失函数中使用。后面使用grad能计算loss相对于神经网络参数的导数。还能，使用jit能加速一切。\n",
    "\n",
    "下面定义效用和损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "  \n",
    "def accuracy(params, images, targets):\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "def loss(params, images, targets):\n",
    "    preds = batched_predict(params, images)\n",
    "    return -jnp.mean(preds * targets)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "    grads = grad(loss)(params, x, y)\n",
    "    return [(w - step_size * dw, b - step_size * db) for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用PyTorch加载数据。JAX专注于程序转换和加速器支持的NumPy，因此JAX库中不包括数据加载或修改。已经有很多出色的数据加载器，所以使用它们而不是重新发明。这里使用PyTorch的数据加载器，并做一个很小的填充以使其与NumPy数组一起使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "class NumpyLoader(data.DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1,\n",
    "                shuffle=False, sampler=None,\n",
    "                batch_sampler=None, num_workers=0,\n",
    "                pin_memory=False, drop_last=False,\n",
    "                timeout=0, worker_init_fn=None):\n",
    "        super(self.__class__, self).__init__(dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=shuffle,\n",
    "                                             sampler=sampler,\n",
    "                                             batch_sampler=batch_sampler,\n",
    "                                             num_workers=num_workers, \n",
    "                                             collate_fn=numpy_collate,\n",
    "                                             pin_memory=pin_memory,\n",
    "                                             drop_last=drop_last,\n",
    "                                             timeout=timeout,\n",
    "                                             worker_init_fn=worker_init_fn)\n",
    "class FlattenAndCast(object):\n",
    "    def __call__(self, pic):\n",
    "        return np.ravel(np.array(pic, dtype=jnp.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: /tmp/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Using downloaded and verified file: /tmp/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%\n",
      "/home/owen411/miniconda3/envs/elks/lib/python3.7/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw\n",
      "\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Define our dataset, using torch datasets\n",
    "# 把图像展开成一维\n",
    "mnist_dataset = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())\n",
    "training_generator = NumpyLoader(mnist_dataset, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owen411/miniconda3/envs/elks/lib/python3.7/site-packages/torchvision/datasets/mnist.py:64: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/home/owen411/miniconda3/envs/elks/lib/python3.7/site-packages/torchvision/datasets/mnist.py:54: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/home/owen411/miniconda3/envs/elks/lib/python3.7/site-packages/torchvision/datasets/mnist.py:69: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/home/owen411/miniconda3/envs/elks/lib/python3.7/site-packages/torchvision/datasets/mnist.py:59: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "# Get the full train dataset (for checking accuracy while training)\n",
    "train_images = np.array(mnist_dataset.train_data).reshape(len(mnist_dataset.train_data), -1)\n",
    "train_labels = one_hot(np.array(mnist_dataset.train_labels), n_targets)\n",
    "\n",
    "# Get full test dataset\n",
    "mnist_dataset_test = MNIST('/tmp/mnist/', download=True, train=False)\n",
    "test_images = jnp.array(mnist_dataset_test.test_data.numpy().reshape(len(mnist_dataset_test.test_data), -1), dtype=jnp.float32)\n",
    "test_labels = one_hot(np.array(mnist_dataset_test.test_labels), n_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 5.54 sec\n",
      "Training set accuracy 0.9158166646957397\n",
      "Test set accuracy 0.919700026512146\n",
      "Epoch 1 in 4.34 sec\n",
      "Training set accuracy 0.9371833205223083\n",
      "Test set accuracy 0.9384999871253967\n",
      "Epoch 2 in 4.59 sec\n",
      "Training set accuracy 0.9492166638374329\n",
      "Test set accuracy 0.9470000267028809\n",
      "Epoch 3 in 4.67 sec\n",
      "Training set accuracy 0.9567166566848755\n",
      "Test set accuracy 0.9532999992370605\n",
      "Epoch 4 in 4.92 sec\n",
      "Training set accuracy 0.9630500078201294\n",
      "Test set accuracy 0.957099974155426\n",
      "Epoch 5 in 4.40 sec\n",
      "Training set accuracy 0.9674500226974487\n",
      "Test set accuracy 0.9617999792098999\n",
      "Epoch 6 in 4.50 sec\n",
      "Training set accuracy 0.97079998254776\n",
      "Test set accuracy 0.9652000069618225\n",
      "Epoch 7 in 4.47 sec\n",
      "Training set accuracy 0.9737833142280579\n",
      "Test set accuracy 0.9672999978065491\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for x, y in training_generator:\n",
    "        y = one_hot(y, n_targets)\n",
    "        params = update(params, x, y)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    train_acc = accuracy(params, train_images, train_labels)\n",
    "    test_acc = accuracy(params, test_images, test_labels)\n",
    "    print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "    print(\"Training set accuracy {}\".format(train_acc))\n",
    "    print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们已经使用了整个JAX API：grad求导，jit加速，vmap自动矢量化。使用NumPy来指定所有的计算，并从PyTorch借用了出色的数据加载器，然后运行了整个程序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From PyTorch to JAX\n",
    "\n",
    "在开始基于JAX的神经网络库之前，有必要了解下 JAX 和 PyTorch之间的区别。\n",
    "\n",
    "关注JAX，主要是因为其高性能的Numpy以及自动微分，即计算某些损失函数相对于输入参数的梯度。但是从PyTorch或Tensorflow 2转到JAX不是一个小变化：建立计算的基本方法，以及更重要的进行反向传播的方式在他们之间是有根本不同的！当前向计算时，PyTorch会建立一个计算图，然后其backward()对某个“结果”节点进行一次调用，然后使用结果节点相对于该中间节点的梯度来加强图中的每个中间节点。而JAX则将计算表示为Python函数，并通过grad()对其进行转换来提供梯度函数，您可以像普通计算函数一样对梯度函数进行求值-输出相对于函数输入的第一个参数（默认）的梯度：\n",
    "\n",
    "![](pictures/comparison_small.png)\n",
    "\n",
    "当然，这会对如何在两个框架中编写代码和建立模型有影响。因此，当习惯于PyTorch或Tensorflow 2中的自动微分且使用有状态对象时，转到JAX可能会有些不习惯。\n",
    "\n",
    "如果查看flax，trax或haiku的库，看到某些神经网络的示例，看起来与其他任何框架也不是太不相似，都是定义一些层，运行一些trainers... 但是里面究竟发生了什么？从小的numpy函数到训练大型分层神经网络的路线是什么？还是值得了解下的。\n",
    "\n",
    "本小节我们将：\n",
    "\n",
    "1. 快速回顾基于反向自动微分框架PyTorch的有状态LSTM-LM的实现，\n",
    "2. 了解PyTorch风格的编码如何依赖于可变状态，了解不可变的纯函数并在JAX中构建（纯）zappy单层代码，\n",
    "3. 将各个参数注册为pytree节点，逐步将它们从单个参数扩展到中等大小的模块，\n",
    "4. 通过构建精美的脚手架并控制上下文以提取初始化参数来净化函数，从而消除增长的痛苦\n",
    "5. 意识到我们可以使用DeepMind的transform机制在诸如DeepMind的haiku框架中轻松实现这一目标。\n",
    "\n",
    "### An LSTM-LM in PyTorch\n",
    "\n",
    "实现一个LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LSTMCell(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.weight_ih = torch.nn.Parameter(torch.rand(4*out_dim, in_dim))\n",
    "        self.weight_hh = torch.nn.Parameter(torch.rand(4*out_dim, out_dim))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(4*out_dim,))\n",
    "        \n",
    "    def forward(self, inputs, h, c):\n",
    "        ifgo = self.weight_ih @ inputs + self.weight_hh @ h + self.bias\n",
    "        i, f, g, o = torch.chunk(ifgo, 4)\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        g = torch.tanh(g)\n",
    "        o = torch.sigmoid(o)\n",
    "        new_c = f * c + i * g\n",
    "        new_h = o * torch.tanh(new_c)\n",
    "        return (new_h, new_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX下的深度学习库\n",
    "\n",
    "根据 https://github.com/google/jax#neural-network-libraries 的说明，有多个Google的研究组使用JAX开发了神经网络库，比如 [Flax](https://github.com/google/flax)，[Trax](https://github.com/google/trax)，[Objax](https://github.com/google/objax)；DeepMind也有[基于JAX的开源生态](https://deepmind.com/blog/article/using-jax-to-accelerate-our-research)，包括[Haiku](https://github.com/deepmind/dm-haiku)，[Optax](https://github.com/deepmind/optax)，[RLax](https://github.com/deepmind/rlax)，以及[Chex](https://github.com/deepmind/chex)\n",
    "\n",
    "如果想要一个具有示例和操作指南的功能齐全的神经网络训练库，请尝试 Flax；另一个选择是 Trax；它是一个基于组合器的框架，专注于易用性和端到端的单命令示例，特别是针对序列模型和强化学习的示例；Objax是具有PyTorch式界面的简约的面向对象框架。\n",
    "\n",
    "Haiku 用于神经网络的模块；Optax梯度处理和优化；RLax为RL算法；Chex用于可靠代码和测试。\n",
    "\n",
    "个人认为选择 Flax 开始更容易上手。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
